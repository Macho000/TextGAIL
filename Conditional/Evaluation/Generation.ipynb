{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.core.global_hydra.GlobalHydra().instance().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'hydra._internal' has no attribute 'GlobalHydra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15139/1895630171.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalHydra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'hydra._internal' has no attribute 'GlobalHydra'"
     ]
    }
   ],
   "source": [
    "hydra._internal.GlobalHydra().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'hydra._internal' has no attribute 'GlobalHydra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15139/3528871502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalHydra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'hydra._internal' has no attribute 'GlobalHydra'"
     ]
    }
   ],
   "source": [
    "hydra._internal.GlobalHydra().get_state().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra.experimental import initialize, compose\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torchfly.text.decode import TransformerDecoder\n",
    "from torchfly.common import set_random_seed, move_to_device\n",
    "\n",
    "from configure_dataloader import DataLoaderHandler\n",
    "from model import Generator, TextGAILModel\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 1\n",
    "set_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.experimental.initialize()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.experimental.initialize(\"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive task/CommonGEN.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive training/CommonGEN.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive model/roberta-tokenized-gpt2.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive text_gail/CommonGEN.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive decode/default.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/omegaconf/basecontainer.py:230: UserWarning: \n",
      "            pretty() is deprecated and will be removed in a future version.\n",
      "            Use OmegaConf.to_yaml. Please note that the default value for\n",
      "            resolve has changed to True.\n",
      "            \n",
      "  elif not resolve and conf._is_interpolation():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:\n",
      "  name: CommonGEN\n",
      "  data_dir: ../../../../data/CommonGEN\n",
      "  mle_weights_path: ''\n",
      "  textgail_weights_path: ''\n",
      "training:\n",
      "  random_seed: 1\n",
      "  num_gpus_per_node: 1\n",
      "  batch_size: 32\n",
      "  resume:\n",
      "    resume: false\n",
      "    resume_model: true\n",
      "    resume_optimizer: true\n",
      "    resume_scheduler: true\n",
      "    resume_rng_state: true\n",
      "  total_num:\n",
      "    epochs: -1\n",
      "    update_steps: 1000\n",
      "  validation:\n",
      "    steps_interval: 100\n",
      "  checkpointing:\n",
      "    directory: Checkpoints\n",
      "    steps_interval: 100\n",
      "    seconds_interval: -1\n",
      "    num_checkpoints_to_keep: 1000\n",
      "    keep_checkpoint_every_num_seconds: 86400\n",
      "  logging:\n",
      "    level: INFO\n",
      "    steps_interval: -1\n",
      "    seconds_interval: 2\n",
      "  optimization:\n",
      "    fp16: false\n",
      "    fp16_opt_level: O1\n",
      "    optimizer_name: AdamW\n",
      "    learning_rate: 1.0e-05\n",
      "    gradient_accumulation_steps: 1\n",
      "    weight_decay: 0.01\n",
      "    max_gradient_norm: -1.0\n",
      "    warmup:\n",
      "      scheduler_name: WarmupLinear\n",
      "      warmup_steps: 100\n",
      "model:\n",
      "  initializer_range: 0.02\n",
      "  layer_norm_epsilon: 1.0e-05\n",
      "  n_ctx: 1024\n",
      "  n_embd: 768\n",
      "  n_head: 12\n",
      "  n_layer: 12\n",
      "  n_positions: 1024\n",
      "  vocab_size: 50265\n",
      "  embd_pdrop: 0.0\n",
      "  resid_pdrop: 0.0\n",
      "  attn_pdrop: 0.0\n",
      "  output_attentions: false\n",
      "  output_hidden_states: false\n",
      "  output_past: true\n",
      "  pad_token_id: 1\n",
      "  name: roberta-tokenized-gpt2\n",
      "text_gail:\n",
      "  batch_size: None\n",
      "  ppo_buffer_size: 128\n",
      "  sample_batch_size: 32\n",
      "  ppo_mini_batch_size: 8\n",
      "  ppo_epoch: 1\n",
      "  ppo_epsilon: 0.2\n",
      "  mix_human_demo_init_ratio: 0.3\n",
      "  mix_human_demo_ratio_warmup_steps: 100\n",
      "  discriminator_pretrain_steps: 400\n",
      "  constant_human_demo_reward: true\n",
      "  recompute_log_probs: true\n",
      "decode:\n",
      "  num_return_sequences: 1\n",
      "  max_steps: 100\n",
      "  do_sample: true\n",
      "  num_beams: 1\n",
      "  temperature: 0.8\n",
      "  top_k: -1\n",
      "  top_p: 0.9\n",
      "  length_penalty: 1.0\n",
      "  bos_token_ids:\n",
      "  - 0\n",
      "  eos_token_ids:\n",
      "  - 2\n",
      "  output_log_probs: false\n",
      "  early_stopping: false\n",
      "  repetition_penalty: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# config = hydra.experimental.compose(\"config.yaml\")\n",
    "hydra.core.global_hydra.GlobalHydra().instance().clear()\n",
    "\n",
    "with initialize(config_path='config'):\n",
    "  config = compose(config_name=\"config\")\n",
    "# cfg = hydra.experimental.compose(config_file='config.yaml', overrides=[])\n",
    "print(config.pretty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': {'name': 'CommonGEN', 'data_dir': '../../../../data/CommonGEN', 'mle_weights_path': '', 'textgail_weights_path': ''}, 'training': {'random_seed': 1, 'num_gpus_per_node': 1, 'batch_size': 32, 'resume': {'resume': False, 'resume_model': True, 'resume_optimizer': True, 'resume_scheduler': True, 'resume_rng_state': True}, 'total_num': {'epochs': -1, 'update_steps': 1000}, 'validation': {'steps_interval': 100}, 'checkpointing': {'directory': 'Checkpoints', 'steps_interval': 100, 'seconds_interval': -1, 'num_checkpoints_to_keep': 1000, 'keep_checkpoint_every_num_seconds': 86400}, 'logging': {'level': 'INFO', 'steps_interval': -1, 'seconds_interval': 2}, 'optimization': {'fp16': False, 'fp16_opt_level': 'O1', 'optimizer_name': 'AdamW', 'learning_rate': 1e-05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'max_gradient_norm': -1.0, 'warmup': {'scheduler_name': 'WarmupLinear', 'warmup_steps': 100}}}, 'model': {'initializer_range': 0.02, 'layer_norm_epsilon': 1e-05, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12, 'n_positions': 1024, 'vocab_size': 50265, 'embd_pdrop': 0.0, 'resid_pdrop': 0.0, 'attn_pdrop': 0.0, 'output_attentions': False, 'output_hidden_states': False, 'output_past': True, 'pad_token_id': 1, 'name': 'roberta-tokenized-gpt2'}, 'text_gail': {'batch_size': 'None', 'ppo_buffer_size': 128, 'sample_batch_size': 32, 'ppo_mini_batch_size': 8, 'ppo_epoch': 1, 'ppo_epsilon': 0.2, 'mix_human_demo_init_ratio': 0.3, 'mix_human_demo_ratio_warmup_steps': 100, 'discriminator_pretrain_steps': 400, 'constant_human_demo_reward': True, 'recompute_log_probs': True}, 'decode': {'num_return_sequences': 1, 'max_steps': 100, 'do_sample': True, 'num_beams': 1, 'temperature': 0.8, 'top_k': -1, 'top_p': 0.9, 'length_penalty': 1.0, 'bos_token_ids': [0], 'eos_token_ids': [2], 'output_log_probs': False, 'early_stopping': False, 'repetition_penalty': 1.0}}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_handler = DataLoaderHandler(config)\n",
    "valid_dataloader = dataloader_handler.test_dataloader(config)\n",
    "collate_fn = test_dataloader.dataset.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGAILModel(config)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(config.decode)\n",
    "decoder.register_generator(model.generator.decoder)\n",
    "decoder.register_tokenizer(tokenizer)\n",
    "decoder.prepare_model_inputs_for_generation = model.generator.prepare_model_inputs_for_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location to store results\n",
    "os.makedirs(config.task.name,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.task.mle_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_weights = torch.load(config.task.mle_weights_path)\n",
    "model.generator.load_state_dict(mle_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = (np.arange(10) + 1) / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temperature in temperatures:\n",
    "    f_write = open(f\"{config.task.name}/mle_{temperature}_{random_seed}.txt\", \"w\")\n",
    "\n",
    "    for batch in tqdm.tqdm(valid_dataloader):\n",
    "        batch = collate_fn(batch)\n",
    "        batch = move_to_device(batch, device)\n",
    "\n",
    "        ground_truth = batch[\"target_text\"]\n",
    "\n",
    "        results = decoder.generate(batch[\"source_token_ids\"], temperature=temperature)\n",
    "        generated = []\n",
    "\n",
    "        for i in range(len(results[\"tokens\"])):\n",
    "            res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "            generated.append(res)\n",
    "\n",
    "        for gt, gen in zip(ground_truth, generated):\n",
    "            f_write.write(json.dumps([gt, gen]))\n",
    "            f_write.write(\"\\n\")\n",
    "        \n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_write = open(f\"{config.task.name}/mle_beam_4.txt\", \"w\")\n",
    "\n",
    "# for batch in tqdm.tqdm(valid_dataloader):\n",
    "#     batch = collate_fn(batch)\n",
    "#     batch = move_to_device(batch, device)\n",
    "\n",
    "#     ground_truth = batch[\"target_text\"]\n",
    "\n",
    "#     results = decoder.generate(batch[\"source_token_ids\"], do_sample=False, num_beams=4)\n",
    "#     generated = []\n",
    "\n",
    "#     for i in range(len(results[\"tokens\"])):\n",
    "#         res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "#         generated.append(res)\n",
    "\n",
    "#     for gt, gen in zip(ground_truth, generated):\n",
    "#         f_write.write(json.dumps([gt, gen]))\n",
    "#         f_write.write(\"\\n\")        \n",
    "\n",
    "# f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextGAIL Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.task.textgail_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgail_weights = torch.load(config.task.textgail_weights_path)\n",
    "model.load_state_dict(textgail_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temperature in temperatures:\n",
    "    f_write = open(f\"{config.task.name}/textgail_{temperature}_{random_seed}.txt\", \"w\")\n",
    "\n",
    "    for batch in tqdm.tqdm(valid_dataloader):\n",
    "        batch = collate_fn(batch)\n",
    "        batch = move_to_device(batch, device)\n",
    "\n",
    "        ground_truth = batch[\"target_text\"]\n",
    "\n",
    "        results = decoder.generate(batch[\"source_token_ids\"], temperature=temperature)\n",
    "        generated = []\n",
    "\n",
    "        for i in range(len(results[\"tokens\"])):\n",
    "            res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "            generated.append(res)\n",
    "\n",
    "        for gt, gen in zip(ground_truth, generated):\n",
    "            f_write.write(json.dumps([gt, gen]))\n",
    "            f_write.write(\"\\n\")\n",
    "        \n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_write = open(f\"{config.task.name}/textgail_no_pretrain2_beam_4.txt\", \"w\")\n",
    "\n",
    "for batch in tqdm.tqdm(valid_dataloader):\n",
    "    batch = collate_fn(batch)\n",
    "    batch = move_to_device(batch, device)\n",
    "\n",
    "    ground_truth = batch[\"target_text\"]\n",
    "\n",
    "    results = decoder.generate(batch[\"source_token_ids\"], do_sample=False, num_beams=4)\n",
    "    generated = []\n",
    "\n",
    "    for i in range(len(results[\"tokens\"])):\n",
    "        res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "        generated.append(res)\n",
    "        \n",
    "    for gt, gen in zip(ground_truth, generated):\n",
    "        f_write.write(json.dumps([gt, gen]))\n",
    "        f_write.write(\"\\n\")        \n",
    "\n",
    "f_write.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
