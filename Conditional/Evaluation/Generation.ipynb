{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import hydra.experimental\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torchfly.text.decode import TransformerDecoder\n",
    "from torchfly.common import set_random_seed, move_to_device\n",
    "\n",
    "from configure_dataloader import DataLoaderHandler\n",
    "from model import Generator, TextGAILModel\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 1\n",
    "set_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.experimental.initialize()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.experimental.initialize(\"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:\n",
      "  name: CommonGEN\n",
      "  data_dir: ../../data/CommonGEN\n",
      "  mle_weights_path: ../MLE/outputs/CommonGEN/Checkpoints/iter_252_model_state.pth\n",
      "  textgail_weights_path: ../TextGAIL/outputs/CommonGEN/Checkpoints/iter_999_model_state.pth\n",
      "training:\n",
      "  random_seed: 1\n",
      "  num_gpus_per_node: 1\n",
      "  batch_size: 32\n",
      "  resume:\n",
      "    resume: false\n",
      "    resume_model: true\n",
      "    resume_optimizer: true\n",
      "    resume_scheduler: true\n",
      "    resume_rng_state: true\n",
      "  total_num:\n",
      "    epochs: -1\n",
      "    update_steps: 1000\n",
      "  validation:\n",
      "    steps_interval: 100\n",
      "  checkpointing:\n",
      "    directory: Checkpoints\n",
      "    steps_interval: 100\n",
      "    seconds_interval: -1\n",
      "    num_checkpoints_to_keep: 1000\n",
      "    keep_checkpoint_every_num_seconds: 86400\n",
      "  logging:\n",
      "    level: INFO\n",
      "    steps_interval: -1\n",
      "    seconds_interval: 2\n",
      "  optimization:\n",
      "    fp16: false\n",
      "    fp16_opt_level: O1\n",
      "    optimizer_name: AdamW\n",
      "    learning_rate: 1.0e-05\n",
      "    gradient_accumulation_steps: 1\n",
      "    weight_decay: 0.01\n",
      "    max_gradient_norm: -1.0\n",
      "    warmup:\n",
      "      scheduler_name: WarmupLinear\n",
      "      warmup_steps: 100\n",
      "model:\n",
      "  initializer_range: 0.02\n",
      "  layer_norm_epsilon: 1.0e-05\n",
      "  n_ctx: 1024\n",
      "  n_embd: 768\n",
      "  n_head: 12\n",
      "  n_layer: 12\n",
      "  n_positions: 1024\n",
      "  vocab_size: 50265\n",
      "  embd_pdrop: 0.0\n",
      "  resid_pdrop: 0.0\n",
      "  attn_pdrop: 0.0\n",
      "  output_attentions: false\n",
      "  output_hidden_states: false\n",
      "  output_past: true\n",
      "  pad_token_id: 1\n",
      "  name: roberta-tokenized-gpt2\n",
      "text_gail:\n",
      "  batch_size: None\n",
      "  ppo_buffer_size: 128\n",
      "  sample_batch_size: 32\n",
      "  ppo_mini_batch_size: 8\n",
      "  ppo_epoch: 1\n",
      "  ppo_epsilon: 0.2\n",
      "  mix_human_demo_init_ratio: 0.3\n",
      "  mix_human_demo_ratio_warmup_steps: 100\n",
      "  discriminator_pretrain_steps: 400\n",
      "  constant_human_demo_reward: true\n",
      "  recompute_log_probs: true\n",
      "decode:\n",
      "  num_return_sequences: 1\n",
      "  max_steps: 100\n",
      "  do_sample: true\n",
      "  num_beams: 1\n",
      "  temperature: 0.8\n",
      "  top_k: -1\n",
      "  top_p: 0.9\n",
      "  length_penalty: 1.0\n",
      "  bos_token_ids:\n",
      "  - 0\n",
      "  eos_token_ids:\n",
      "  - 2\n",
      "  output_log_probs: false\n",
      "  early_stopping: false\n",
      "  repetition_penalty: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive task/CommonGEN.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive training/CommonGEN.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive model/roberta-tokenized-gpt2.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive text_gail/CommonGEN.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive decode/default.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n"
     ]
    }
   ],
   "source": [
    "config = hydra.experimental.compose(\"config.yaml\")\n",
    "print(config.pretty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_handler = DataLoaderHandler(config)\n",
    "test_dataloader = dataloader_handler.test_dataloader(config)\n",
    "collate_fn = test_dataloader.dataset.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGAILModel(config)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(config.decode)\n",
    "decoder.register_generator(model.generator.decoder)\n",
    "decoder.register_tokenizer(tokenizer)\n",
    "decoder.prepare_model_inputs_for_generation = model.generator.prepare_model_inputs_for_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location to store results\n",
    "os.makedirs(config.task.name,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../MLE/outputs/CommonGEN/Checkpoints/iter_252_model_state.pth\n"
     ]
    }
   ],
   "source": [
    "print(config.task.mle_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_weights = torch.load(config.task.mle_weights_path)\n",
    "model.generator.load_state_dict(mle_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = (np.arange(10) + 1) / 10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [01:01<00:00,  3.05it/s]\n",
      "100%|██████████| 189/189 [01:11<00:00,  2.64it/s]\n",
      "100%|██████████| 189/189 [01:18<00:00,  2.41it/s]\n",
      "100%|██████████| 189/189 [01:45<00:00,  1.79it/s]\n",
      "100%|██████████| 189/189 [02:07<00:00,  1.48it/s]\n",
      "100%|██████████| 189/189 [02:29<00:00,  1.27it/s]\n",
      " 52%|█████▏    | 99/189 [01:32<01:23,  1.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8429/4112638646.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source_token_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torchfly/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/TextGAIL/TorchFly/torchfly/text/decode/transformer_decoder.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, model_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_beams\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_no_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/TextGAIL/TorchFly/torchfly/text/decode/transformer_decoder.py\u001b[0m in \u001b[0;36m_generate_no_beam_search\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mgenerated_token_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerated_token_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mgenerated_log_prob_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerated_log_prob_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             )\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/TextGAIL/TorchFly/torchfly/text/decode/transformer_decoder.py\u001b[0m in \u001b[0;36mcompute_logits\u001b[0;34m(self, timestep, model_inputs, generated_token_sequences, generated_log_prob_sequences)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# generate next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torchfly/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/TextGAIL/TorchFly/torchfly/nn/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         )\n\u001b[1;32m    386\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torchfly/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/TextGAIL/TorchFly/torchfly/nn/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, position_ids, token_type_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torchfly/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/TextGAIL/TorchFly/torchfly/nn/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0moutput_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_attn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torchfly/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for temperature in temperatures:\n",
    "    f_write = open(f\"{config.task.name}/mle_{temperature}_{random_seed}.txt\", \"w\")\n",
    "\n",
    "    for batch in tqdm.tqdm(valid_dataloader):\n",
    "        batch = collate_fn(batch)\n",
    "        batch = move_to_device(batch, device)\n",
    "\n",
    "        ground_truth = batch[\"target_text\"]\n",
    "\n",
    "        results = decoder.generate(batch[\"source_token_ids\"], temperature=temperature)\n",
    "        generated = []\n",
    "\n",
    "        for i in range(len(results[\"tokens\"])):\n",
    "            res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "            generated.append(res)\n",
    "\n",
    "        for gt, gen in zip(ground_truth, generated):\n",
    "            f_write.write(json.dumps([gt, gen]))\n",
    "            f_write.write(\"\\n\")\n",
    "        \n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_write = open(f\"{config.task.name}/mle_beam_4.txt\", \"w\")\n",
    "\n",
    "# for batch in tqdm.tqdm(valid_dataloader):\n",
    "#     batch = collate_fn(batch)\n",
    "#     batch = move_to_device(batch, device)\n",
    "\n",
    "#     ground_truth = batch[\"target_text\"]\n",
    "\n",
    "#     results = decoder.generate(batch[\"source_token_ids\"], do_sample=False, num_beams=4)\n",
    "#     generated = []\n",
    "\n",
    "#     for i in range(len(results[\"tokens\"])):\n",
    "#         res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "#         generated.append(res)\n",
    "\n",
    "#     for gt, gen in zip(ground_truth, generated):\n",
    "#         f_write.write(json.dumps([gt, gen]))\n",
    "#         f_write.write(\"\\n\")        \n",
    "\n",
    "# f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextGAIL Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.task.textgail_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgail_weights = torch.load(config.task.textgail_weights_path)\n",
    "model.load_state_dict(textgail_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temperature in temperatures:\n",
    "    f_write = open(f\"{config.task.name}/textgail_{temperature}_{random_seed}.txt\", \"w\")\n",
    "\n",
    "    for batch in tqdm.tqdm(valid_dataloader):\n",
    "        batch = collate_fn(batch)\n",
    "        batch = move_to_device(batch, device)\n",
    "\n",
    "        ground_truth = batch[\"target_text\"]\n",
    "\n",
    "        results = decoder.generate(batch[\"source_token_ids\"], temperature=temperature)\n",
    "        generated = []\n",
    "\n",
    "        for i in range(len(results[\"tokens\"])):\n",
    "            res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "            generated.append(res)\n",
    "\n",
    "        for gt, gen in zip(ground_truth, generated):\n",
    "            f_write.write(json.dumps([gt, gen]))\n",
    "            f_write.write(\"\\n\")\n",
    "        \n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_write = open(f\"{config.task.name}/textgail_no_pretrain2_beam_4.txt\", \"w\")\n",
    "\n",
    "for batch in tqdm.tqdm(valid_dataloader):\n",
    "    batch = collate_fn(batch)\n",
    "    batch = move_to_device(batch, device)\n",
    "\n",
    "    ground_truth = batch[\"target_text\"]\n",
    "\n",
    "    results = decoder.generate(batch[\"source_token_ids\"], do_sample=False, num_beams=4)\n",
    "    generated = []\n",
    "\n",
    "    for i in range(len(results[\"tokens\"])):\n",
    "        res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "        generated.append(res)\n",
    "        \n",
    "    for gt, gen in zip(ground_truth, generated):\n",
    "        f_write.write(json.dumps([gt, gen]))\n",
    "        f_write.write(\"\\n\")        \n",
    "\n",
    "f_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'hydra._internal' has no attribute 'GlobalHydra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15139/3528871502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalHydra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'hydra._internal' has no attribute 'GlobalHydra'"
     ]
    }
   ],
   "source": [
    "hydra._internal.GlobalHydra().get_state().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra.experimental import initialize, compose\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torchfly.text.decode import TransformerDecoder\n",
    "from torchfly.common import set_random_seed, move_to_device\n",
    "\n",
    "from configure_dataloader import DataLoaderHandler\n",
    "from model import Generator, TextGAILModel\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 1\n",
    "set_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.experimental.initialize()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.experimental.initialize(\"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive task/CommonGEN.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive training/CommonGEN.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive model/roberta-tokenized-gpt2.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive text_gail/CommonGEN.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive decode/default.yaml in file:///home/aigo/TextGAIL/Conditional/Evaluation/config.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/aigo/anaconda3/envs/torchfly/lib/python3.7/site-packages/omegaconf/basecontainer.py:230: UserWarning: \n",
      "            pretty() is deprecated and will be removed in a future version.\n",
      "            Use OmegaConf.to_yaml. Please note that the default value for\n",
      "            resolve has changed to True.\n",
      "            \n",
      "  elif not resolve and conf._is_interpolation():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:\n",
      "  name: CommonGEN\n",
      "  data_dir: ../../../../data/CommonGEN\n",
      "  mle_weights_path: ''\n",
      "  textgail_weights_path: ''\n",
      "training:\n",
      "  random_seed: 1\n",
      "  num_gpus_per_node: 1\n",
      "  batch_size: 32\n",
      "  resume:\n",
      "    resume: false\n",
      "    resume_model: true\n",
      "    resume_optimizer: true\n",
      "    resume_scheduler: true\n",
      "    resume_rng_state: true\n",
      "  total_num:\n",
      "    epochs: -1\n",
      "    update_steps: 1000\n",
      "  validation:\n",
      "    steps_interval: 100\n",
      "  checkpointing:\n",
      "    directory: Checkpoints\n",
      "    steps_interval: 100\n",
      "    seconds_interval: -1\n",
      "    num_checkpoints_to_keep: 1000\n",
      "    keep_checkpoint_every_num_seconds: 86400\n",
      "  logging:\n",
      "    level: INFO\n",
      "    steps_interval: -1\n",
      "    seconds_interval: 2\n",
      "  optimization:\n",
      "    fp16: false\n",
      "    fp16_opt_level: O1\n",
      "    optimizer_name: AdamW\n",
      "    learning_rate: 1.0e-05\n",
      "    gradient_accumulation_steps: 1\n",
      "    weight_decay: 0.01\n",
      "    max_gradient_norm: -1.0\n",
      "    warmup:\n",
      "      scheduler_name: WarmupLinear\n",
      "      warmup_steps: 100\n",
      "model:\n",
      "  initializer_range: 0.02\n",
      "  layer_norm_epsilon: 1.0e-05\n",
      "  n_ctx: 1024\n",
      "  n_embd: 768\n",
      "  n_head: 12\n",
      "  n_layer: 12\n",
      "  n_positions: 1024\n",
      "  vocab_size: 50265\n",
      "  embd_pdrop: 0.0\n",
      "  resid_pdrop: 0.0\n",
      "  attn_pdrop: 0.0\n",
      "  output_attentions: false\n",
      "  output_hidden_states: false\n",
      "  output_past: true\n",
      "  pad_token_id: 1\n",
      "  name: roberta-tokenized-gpt2\n",
      "text_gail:\n",
      "  batch_size: None\n",
      "  ppo_buffer_size: 128\n",
      "  sample_batch_size: 32\n",
      "  ppo_mini_batch_size: 8\n",
      "  ppo_epoch: 1\n",
      "  ppo_epsilon: 0.2\n",
      "  mix_human_demo_init_ratio: 0.3\n",
      "  mix_human_demo_ratio_warmup_steps: 100\n",
      "  discriminator_pretrain_steps: 400\n",
      "  constant_human_demo_reward: true\n",
      "  recompute_log_probs: true\n",
      "decode:\n",
      "  num_return_sequences: 1\n",
      "  max_steps: 100\n",
      "  do_sample: true\n",
      "  num_beams: 1\n",
      "  temperature: 0.8\n",
      "  top_k: -1\n",
      "  top_p: 0.9\n",
      "  length_penalty: 1.0\n",
      "  bos_token_ids:\n",
      "  - 0\n",
      "  eos_token_ids:\n",
      "  - 2\n",
      "  output_log_probs: false\n",
      "  early_stopping: false\n",
      "  repetition_penalty: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# config = hydra.experimental.compose(\"config.yaml\")\n",
    "hydra.core.global_hydra.GlobalHydra().instance().clear()\n",
    "\n",
    "with initialize(config_path='config'):\n",
    "  config = compose(config_name=\"config\")\n",
    "# cfg = hydra.experimental.compose(config_file='config.yaml', overrides=[])\n",
    "print(config.pretty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': {'name': 'CommonGEN', 'data_dir': '../../../../data/CommonGEN', 'mle_weights_path': '', 'textgail_weights_path': ''}, 'training': {'random_seed': 1, 'num_gpus_per_node': 1, 'batch_size': 32, 'resume': {'resume': False, 'resume_model': True, 'resume_optimizer': True, 'resume_scheduler': True, 'resume_rng_state': True}, 'total_num': {'epochs': -1, 'update_steps': 1000}, 'validation': {'steps_interval': 100}, 'checkpointing': {'directory': 'Checkpoints', 'steps_interval': 100, 'seconds_interval': -1, 'num_checkpoints_to_keep': 1000, 'keep_checkpoint_every_num_seconds': 86400}, 'logging': {'level': 'INFO', 'steps_interval': -1, 'seconds_interval': 2}, 'optimization': {'fp16': False, 'fp16_opt_level': 'O1', 'optimizer_name': 'AdamW', 'learning_rate': 1e-05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'max_gradient_norm': -1.0, 'warmup': {'scheduler_name': 'WarmupLinear', 'warmup_steps': 100}}}, 'model': {'initializer_range': 0.02, 'layer_norm_epsilon': 1e-05, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12, 'n_positions': 1024, 'vocab_size': 50265, 'embd_pdrop': 0.0, 'resid_pdrop': 0.0, 'attn_pdrop': 0.0, 'output_attentions': False, 'output_hidden_states': False, 'output_past': True, 'pad_token_id': 1, 'name': 'roberta-tokenized-gpt2'}, 'text_gail': {'batch_size': 'None', 'ppo_buffer_size': 128, 'sample_batch_size': 32, 'ppo_mini_batch_size': 8, 'ppo_epoch': 1, 'ppo_epsilon': 0.2, 'mix_human_demo_init_ratio': 0.3, 'mix_human_demo_ratio_warmup_steps': 100, 'discriminator_pretrain_steps': 400, 'constant_human_demo_reward': True, 'recompute_log_probs': True}, 'decode': {'num_return_sequences': 1, 'max_steps': 100, 'do_sample': True, 'num_beams': 1, 'temperature': 0.8, 'top_k': -1, 'top_p': 0.9, 'length_penalty': 1.0, 'bos_token_ids': [0], 'eos_token_ids': [2], 'output_log_probs': False, 'early_stopping': False, 'repetition_penalty': 1.0}}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_handler = DataLoaderHandler(config)\n",
    "valid_dataloader = dataloader_handler.test_dataloader(config)\n",
    "collate_fn = test_dataloader.dataset.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGAILModel(config)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(config.decode)\n",
    "decoder.register_generator(model.generator.decoder)\n",
    "decoder.register_tokenizer(tokenizer)\n",
    "decoder.prepare_model_inputs_for_generation = model.generator.prepare_model_inputs_for_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location to store results\n",
    "os.makedirs(config.task.name,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.task.mle_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_weights = torch.load(config.task.mle_weights_path)\n",
    "model.generator.load_state_dict(mle_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = (np.arange(10) + 1) / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temperature in temperatures:\n",
    "    f_write = open(f\"{config.task.name}/mle_{temperature}_{random_seed}.txt\", \"w\")\n",
    "\n",
    "    for batch in tqdm.tqdm(valid_dataloader):\n",
    "        batch = collate_fn(batch)\n",
    "        batch = move_to_device(batch, device)\n",
    "\n",
    "        ground_truth = batch[\"target_text\"]\n",
    "\n",
    "        results = decoder.generate(batch[\"source_token_ids\"], temperature=temperature)\n",
    "        generated = []\n",
    "\n",
    "        for i in range(len(results[\"tokens\"])):\n",
    "            res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "            generated.append(res)\n",
    "\n",
    "        for gt, gen in zip(ground_truth, generated):\n",
    "            f_write.write(json.dumps([gt, gen]))\n",
    "            f_write.write(\"\\n\")\n",
    "        \n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_write = open(f\"{config.task.name}/mle_beam_4.txt\", \"w\")\n",
    "\n",
    "# for batch in tqdm.tqdm(valid_dataloader):\n",
    "#     batch = collate_fn(batch)\n",
    "#     batch = move_to_device(batch, device)\n",
    "\n",
    "#     ground_truth = batch[\"target_text\"]\n",
    "\n",
    "#     results = decoder.generate(batch[\"source_token_ids\"], do_sample=False, num_beams=4)\n",
    "#     generated = []\n",
    "\n",
    "#     for i in range(len(results[\"tokens\"])):\n",
    "#         res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "#         generated.append(res)\n",
    "\n",
    "#     for gt, gen in zip(ground_truth, generated):\n",
    "#         f_write.write(json.dumps([gt, gen]))\n",
    "#         f_write.write(\"\\n\")        \n",
    "\n",
    "# f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextGAIL Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.task.textgail_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgail_weights = torch.load(config.task.textgail_weights_path)\n",
    "model.load_state_dict(textgail_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temperature in temperatures:\n",
    "    f_write = open(f\"{config.task.name}/textgail_{temperature}_{random_seed}.txt\", \"w\")\n",
    "\n",
    "    for batch in tqdm.tqdm(valid_dataloader):\n",
    "        batch = collate_fn(batch)\n",
    "        batch = move_to_device(batch, device)\n",
    "\n",
    "        ground_truth = batch[\"target_text\"]\n",
    "\n",
    "        results = decoder.generate(batch[\"source_token_ids\"], temperature=temperature)\n",
    "        generated = []\n",
    "\n",
    "        for i in range(len(results[\"tokens\"])):\n",
    "            res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "            generated.append(res)\n",
    "\n",
    "        for gt, gen in zip(ground_truth, generated):\n",
    "            f_write.write(json.dumps([gt, gen]))\n",
    "            f_write.write(\"\\n\")\n",
    "        \n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_write = open(f\"{config.task.name}/textgail_no_pretrain2_beam_4.txt\", \"w\")\n",
    "\n",
    "for batch in tqdm.tqdm(valid_dataloader):\n",
    "    batch = collate_fn(batch)\n",
    "    batch = move_to_device(batch, device)\n",
    "\n",
    "    ground_truth = batch[\"target_text\"]\n",
    "\n",
    "    results = decoder.generate(batch[\"source_token_ids\"], do_sample=False, num_beams=4)\n",
    "    generated = []\n",
    "\n",
    "    for i in range(len(results[\"tokens\"])):\n",
    "        res = tokenizer.decode(results[\"tokens\"][i][0][1:-1].tolist())\n",
    "        generated.append(res)\n",
    "        \n",
    "    for gt, gen in zip(ground_truth, generated):\n",
    "        f_write.write(json.dumps([gt, gen]))\n",
    "        f_write.write(\"\\n\")        \n",
    "\n",
    "f_write.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
