training:
    fp16: False
    random_seed: 1
    num_gpus_per_node: 1
    batch_size: 32
    resume:
        resume: False
    checkpointing:
        directory: "Checkpoints"
        steps_interval: 10
        seconds_interval: -1
        num_checkpoints_to_keep: 1000
        keep_checkpoint_every_num_seconds: 86400
    optimization:
        optimizer_name: AdamW
        learning_rate: 1e-5
        weight_decay: 0.01
        max_gradient_norm: 1.0
    scheduler:
        scheduler_name: WarmupLinear
        warmup_steps: 300
    evaluation:
        steps_interval: 10 # after every epoch or none 
    total_num:
        epochs: -1
        update_steps: 1000 # disabled when total_num_epochs > 0